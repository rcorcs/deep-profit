\section{Introduction}
\label{sec:introduction}

In recent years, the market for mobile and embedded systems has been rapidly growing.
These systems must often run on inexpensive and resource-constrained devices, with limited memory, storage, CPU caches. %and their applications are designed with different goals compared to traditional computing systems.
Applications for these systems are designed and built with different goals compared to traditional computing systems since their binaries must fit in a budget memory size.
%One way to deal with limited memory is to develop compilation techniques which generate small binaries.
Hence, compilation techniques must be primarily focused on optimising for binary size.

One important optimisation capable of reducing code size is function merging.
In its simplest form, function merging reduces replicated code by combining multiple identical functions into a single one~\cite{llvm-fm,livska14}.
More advanced approaches can identify similar, but not necessarily identical, functions and replace them with a single function that combines the functionality of the original functions while eliminating redundant code.
At a high level, the way this works is that code specific to only one input function is added to the merged function but made conditional to a function identifier, while code found in both input functions is added only once and executed regardless of the function identifier.

A recent work has generalized function merging to work on arbitrary pair of functions.
The state-of-the-art~\cite{rocha19, rocha20} technique works in three major phases:
First, it represents functions as nothing more than linear sequences of instructions.
Then it applies a sequence alignment algorithm, developed for bioinformatics, to discover the optimal way to create pairs of mergeable instructions from the two input sequences.
Finally, it performs code generation, producing the merged function where aligned pairs of matching instructions are merged to a single instruction, while non-matching instructions are simply copied into the merged function.

The state-of-the-art optimization also includes a search strategy based on ranking the function candidates with higher similarity.
However, because this strategy is unable to decide which one of those pairs are actually worth merging, the optimizer uses the compiler's built-in cost model in its profitability analysis.
This analysis is responsible to decide which merged functions should be kept, replacing the original functions.
The profitability is estimated based on a target-specific cost model that assigns weights to instructions.

In this paper, we expose innaccuracies in this profitability analysis, showing there are still opportunities for further improvement.
We propose a new approach for the profitability analysis based on partial recompilation.
With this technique, the estimated reduction in code size more closely resembles the actual reduction observed in the final object file.

Moreover, we show that most merged functions are actually unprofitable.
Even if we consider only the top-ranked candidate functions, about 82\% of the candidate functions are still unprofitably merged.
Since the function merging operation is computationally expensive, we should avoid wasting time with unprofitable merged functions, freeing the compiler to focus more its efforts optimizing code that more likely to be profitable.
In order to address this issue, we also describe our heuristic model based on deep-learning that predicts whether or not a pair of functions can be profitably merged, avoiding wasteful merge operations.

%This allows us to avoid merging pairs of functions that are unlikely to result in a profitable merge operation.

